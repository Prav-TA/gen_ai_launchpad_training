{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd62ba99",
   "metadata": {},
   "source": [
    "#### Part -1 : Build a Retrieval Augmented Generation (RAG) App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a6adb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from typing import List, TypedDict, Literal\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain import hub\n",
    "from PIL import Image \n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574ee40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "os.environ['LANGSMITH_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dae604",
   "metadata": {},
   "source": [
    "#### Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93350ccb",
   "metadata": {},
   "source": [
    "##### Chat model : NVIDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e2d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"NVIDIA_API_KEY\"):\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1eb4c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"meta/llama3-70b-instruct\", model_provider=\"nvidia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6035f76",
   "metadata": {},
   "source": [
    "##### Embedding model : NVIDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aaf3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"NVIDIA_API_KEY\"):\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b350d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a261c",
   "metadata": {},
   "source": [
    "##### Vector Store : InMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f88079a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b166a7a",
   "metadata": {},
   "source": [
    "##### RAG chain pipeline with query analysis using langGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127574b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_characters: 43130\n",
      "Split the blog post into 66 chunks\n",
      "Total chunks indexed & added to the vector store: 66\n",
      "Response: Task Decomposition is the process of breaking down complex tasks into smaller, simpler, and manageable steps. This process involves utilizing more test-time computation to transform big tasks into multiple manageable tasks. It helps in interpreting the model's thinking process and enhancing model performance on complex tasks.\n",
      "{'analyze_query': {'query': Search(query='Task Decomposition', section='beginning')}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'retrieve': {'context': [Document(id='3060cc1e-adf4-4ae5-b44d-e6be7ceb0105', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585, 'section': 'beginning'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='202ee5f3-097d-4ec5-9090-c560cdd874f2', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585, 'section': 'beginning'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='b5b520c5-7140-4283-bcb8-39fe4074c270', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585, 'section': 'beginning'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='81c8019c-ee00-44fe-ad58-1bcbae08e2b0', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585, 'section': 'beginning'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': \"Task Decomposition is the process of breaking down complex tasks into smaller and simpler steps, making them more manageable. This allows agents to plan ahead and think step by step to tackle complicated tasks. It transforms big tasks into multiple smaller tasks, providing insight into the model's thinking process.\"}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the blog post using WebBaseLoader\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "assert len(docs) == 1\n",
    "print(f\"Total_characters: {len(docs[0].page_content)}\")\n",
    "\n",
    "# Split the blog post into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"Split the blog post into {len(all_splits)} chunks\")\n",
    "\n",
    "# Add metadata to each chunk\n",
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for idx, doc in enumerate(all_splits):\n",
    "    if idx < third:\n",
    "        doc.metadata[\"section\"] = \"beginning\"\n",
    "    elif idx < third * 2:\n",
    "        doc.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        doc.metadata[\"section\"] = \"end\"\n",
    "\n",
    "# Add the chunks to the vector store\n",
    "document_ids = vector_store.add_documents(all_splits)\n",
    "print(\"Total chunks indexed & added to the vector store:\", len(document_ids))\n",
    "\n",
    "# Define search schema\n",
    "class Search(BaseModel):\n",
    "    query: str = Field(..., description=\"Search query to run.\")\n",
    "    section: Literal[\"beginning\", \"middle\", \"end\"] = Field(\n",
    "        ..., description=\"Section to query.\"\n",
    "    )  # Field(...) says this is a required field # Literal is a type alias that represents a fixed set of values\n",
    "\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define state\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Define analyze_query node\n",
    "def analyze_query(state: State):\n",
    "\n",
    "   # Custom prompt for structured query extraction\n",
    "    system_msg = (\n",
    "        \"You are a helpful assistant that extracts structured data from questions. \"\n",
    "        \"Given a user's question, extract the search query and whether the answer should be \"\n",
    "        \"from the beginning, middle, or end of a document. \"\n",
    "        \"Respond strictly in this JSON format:\\n\"\n",
    "        '{ \"query\": \"<search terms>\", \"section\": \"beginning|middle|end\" }'\n",
    "            )\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system_msg),\n",
    "        HumanMessage(content=state[\"question\"]),\n",
    "    ]\n",
    "\n",
    "    # Now bind the LLM with only the schema (not a prompt)\n",
    "    structured_llm = llm.with_structured_output(schema = Search)\n",
    "    query = structured_llm.invoke(messages)\n",
    "    return {\"query\": query}\n",
    "\n",
    "\n",
    "# Define retrieve node\n",
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query.query,\n",
    "        filter=lambda doc: doc.metadata.get(\"section\") == query.section,\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "# Define generate node\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Build LangGraph\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "\n",
    "# Run the graph\n",
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(\"Response:\",response[\"answer\"])\n",
    "\n",
    "\n",
    "# Graph stream in steps\n",
    "for step in graph.stream(\n",
    "    {\"question\": \"What is Task Decomposition?\"},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaf7842",
   "metadata": {},
   "source": [
    "##### Other qns and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94444b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Lilian Weng is the author of the article \"LLM Powered Autonomous Agents\" published on June 23, 2023. She is likely an expert in the field of large language models and their applications in autonomous agents. No further information about her is provided in the given context.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Who is Lilian Weng?\"})\n",
    "print(\"Response:\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003fe052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I don't know. The provided context does not mention the current prime minister of India.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Who is the current prime minister of India?\"})\n",
    "print(\"Response:\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "227368e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I don't know what LangSmith is, as it is not mentioned in the provided context. The context only talks about LSH (Locality-Sensitive Hashing) and ANNOY (Approximate Nearest Neighbors Oh Yeah).\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is LangSmith?\"})\n",
    "print(\"Response:\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a6e0dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response1: Task Decomposition is the process of breaking down complex tasks into smaller and simpler steps, making them more manageable. This allows agents or models to plan ahead and utilize more test-time computation to complete the task. It transforms big tasks into multiple manageable tasks, providing insight into the model's thinking process.\n",
      "Response2: I don't know what \"it\" refers to in the question, as the context does not provide a specific topic or action being discussed.\n"
     ]
    }
   ],
   "source": [
    "response1 = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "response2 = graph.invoke({\"question\": \"What are common ways of doing it?\"})\n",
    "print(\"Response1:\", response1[\"answer\"])\n",
    "print(\"Response2:\", response2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de652799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
